{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from scipy.stats import entropy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_col = 'Excerpt Copy'\n",
    "\n",
    "df = pd.read_csv('data/coder1_all.tsv', sep='\\t')\n",
    "df = df[['uni', 'Participant', 'Excerpt Copy', 'rank', 'identity',\n",
    "       'Q3-g', 'Q3-l', 'Q3-b', 'Q3-quest', 'Q3-ace', 'Q3-queer', 'Q4-gq',\n",
    "       'Q4-t', 'Q4-i', 'Q4-f', 'Q4-m']]\n",
    "\n",
    "print(df.shape[0])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.replace({'Question: Q\\d*\\w?; Answer:': ''}, regex=True)\n",
    "df = df.replace({'Question: Q\\d*-other; Answer:': ''}, regex=True)\n",
    "\n",
    "def unlist(x):\n",
    "    return x[0]\n",
    "\n",
    "text = df[['uni', 'Participant', 'Excerpt Copy']].groupby(['uni', 'Participant'])\n",
    "text = text.agg(lambda t: \"%s\" % ' '.join(t))\n",
    "text = text['Excerpt Copy']\n",
    "print(text.shape[0])\n",
    "text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_topics = 10\n",
    "n_snow = 10\n",
    "\n",
    "documents = text.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        words = \" \".join([feature_names[i] \n",
    "                          for i in topic.argsort()[:-no_top_words - 1:-1]])\n",
    "        print(\"Topic\", topic_idx, \":  \", words)\n",
    "\n",
    "def JSD(P, Q):\n",
    "    _P = P / np.linalg.norm(P, ord=1)\n",
    "    _Q = Q / np.linalg.norm(Q, ord=1)\n",
    "    _M = 0.5 * (_P + _Q)\n",
    "    return 0.5 * (entropy(_P, _M) + entropy(_Q, _M))\n",
    "\n",
    "def list_sims(df):\n",
    "    n = df.shape[0]\n",
    "    result = []\n",
    "    \n",
    "    for i in range(0,n):\n",
    "        for j in range(i+1,n):\n",
    "            tmp = {'i': i, 'j': j, 'jsd': JSD(df.loc[i], df.loc[j])}\n",
    "            result.append(tmp)\n",
    "    \n",
    "    return pd.DataFrame(result)\n",
    "    \n",
    "def worker(documents, method='NMF', n_topics=10, calc_edges=True):   \n",
    "    if method == 'NMF':\n",
    "        vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, \n",
    "                                       max_features=1000, \n",
    "                                       stop_words='english')\n",
    "        mod = NMF(n_components=n_topics, \n",
    "                      alpha=.1, \n",
    "                      l1_ratio=.5, \n",
    "                      init='nndsvd')\n",
    "        \n",
    "    elif method == 'LDA':\n",
    "        vectorizer = CountVectorizer(max_df=0.95, min_df=2, \n",
    "                                    max_features=1000, \n",
    "                                    stop_words='english')\n",
    "        mod = LatentDirichletAllocation(n_components=n_topics, \n",
    "                                    max_iter=20, \n",
    "                                    learning_method='online', \n",
    "                                    n_jobs=-1 )\n",
    "\n",
    "    transformed = vectorizer.fit_transform(documents)\n",
    "    feat_names = vectorizer.get_feature_names()\n",
    "    model = mod.fit(transformed)\n",
    "    \n",
    "    display_topics(model, feat_names, n_snow)\n",
    "    \n",
    "    edges = None\n",
    "    if calc_edges:\n",
    "        edges = list_sims(transformed)\n",
    "    \n",
    "    return edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "person = text.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = {}\n",
    "edges['nmf_person'] = worker(person, 'NMF')\n",
    "edges['lda_person'] = worker(person, 'LDA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges['nmf_person'].jsd.hist(bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges['lda_person'].jsd.hist(bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [3, 5, 8, 10, 15]:\n",
    "    print(\"\\n\\nNMF\", i)\n",
    "    worker(person, 'NMF', n_topics=i, calc_edges=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [3, 5, 8, 10, 15]:\n",
    "    print(\"\\n\\nLDA:\", i)\n",
    "    worker(person, 'LDA', n_topics=i, calc_edges=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp.to_csv('data/cosine_people.tsv', sep='\\t')\n",
    "text=text[['uni', 'Participant']]\n",
    "text.to_csv('data/cosine_people_ids.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TfidfVectorizer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import regexp_tokenize \n",
    "\n",
    "stemmer = SnowballStemmer(\"english\") \n",
    "\n",
    "def my_tokenizer(text):\n",
    "    out = []\n",
    "    for w in regexp_tokenize(text, '\\w+'):\n",
    "        out.append(stemmer.stem(w))\n",
    "    return out\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, \n",
    "                max_features=1000, \n",
    "                stop_words='english',\n",
    "                tokenizer=my_tokenizer\n",
    "               ).fit(text.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = NMF(n_components=15, \n",
    "              alpha=.1, \n",
    "              l1_ratio=.5, \n",
    "              init='nndsvd')\n",
    "\n",
    "transformed = vectorizer.fit_transform(text.values)\n",
    "feat_names = vectorizer.get_feature_names()\n",
    "model = mod.fit(transformed)\n",
    "\n",
    "display_topics(model, feat_names, n_snow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "worker(person, 'NMF', n_topics=15, calc_edges=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
